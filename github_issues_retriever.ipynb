{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c6515b",
   "metadata": {},
   "source": [
    "# GitHub Issues Retriever\n",
    "\n",
    "This notebook demonstrates how to retrieve issues from a specific GitHub repository using the GitHub API. The issues will be filtered with the criteria `is:issue is:closed label:bug linked:pr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757bc73e-9884-46b7-8d1a-59b3f7e49ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\usuario\\miniconda3\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (0.3.49)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (2.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\usuario\\miniconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: openai in c:\\users\\usuario\\miniconda3\\lib\\site-packages (1.68.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\usuario\\miniconda3\\lib\\site-packages (0.3.11)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-openai) (0.3.49)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-openai) (1.68.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.3.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openpyxl\n",
    "!pip install openai\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef99479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import os\n",
    "from typing import Dict, Any, Set, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from openpyxl import Workbook, load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab9156-117d-4fbb-84da-1b520c3c720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "GITHUB_API_URL = os.getenv('GITHUB_API_URL', 'https://api.github.com')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANALYZED_ISSUES_FILE = os.getenv('ANALYZED_ISSUES_FILE', 'analyzed_issues.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6499c3d-23ce-408c-9bc0-e555d0310a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzedIssuesStorage:\n",
    "    def __init__(self, filename=ANALYZED_ISSUES_FILE):\n",
    "        self.filename = filename\n",
    "        self.analyzed_issues = {}\n",
    "        self.visited_pages = {}\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        if not os.path.exists(self.filename):\n",
    "            workbook = Workbook()\n",
    "            sheet = workbook.active\n",
    "            sheet.title = 'Analyzed Issues'\n",
    "            sheet.append(['issue_id', 'repo_owner', 'repo_name', 'issue_description', 'issue_prompt', \n",
    "                         'issue_url', 'issue_state', 'issue_files', 'issue_pr_url'])\n",
    "            workbook.save(self.filename)\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            workbook = load_workbook(self.filename)\n",
    "            sheet = workbook.active\n",
    "            \n",
    "            self.analyzed_issues = {}\n",
    "            for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "                if not row[0]:  \n",
    "                    continue\n",
    "                \n",
    "                issue_id, owner, repo = row[0], row[1], row[2]\n",
    "                if owner not in self.analyzed_issues:\n",
    "                    self.analyzed_issues[owner] = {}\n",
    "                if repo not in self.analyzed_issues[owner]:\n",
    "                    self.analyzed_issues[owner][repo] = []\n",
    "                self.analyzed_issues[owner][repo].append(str(issue_id))\n",
    "            \n",
    "            self.visited_pages = {}\n",
    "            visited_pages_sheet = None\n",
    "            \n",
    "            if 'Visited Pages' in workbook.sheetnames:\n",
    "                visited_pages_sheet = workbook['Visited Pages']\n",
    "            else:\n",
    "                visited_pages_sheet = workbook.create_sheet('Visited Pages')\n",
    "                visited_pages_sheet.append(['repo_owner', 'repo_name', 'page_number'])\n",
    "                workbook.save(self.filename)\n",
    "            \n",
    "            for row in visited_pages_sheet.iter_rows(min_row=2, values_only=True):\n",
    "                if not row[0]:\n",
    "                    continue\n",
    "                \n",
    "                owner, repo, page = row[0], row[1], row[2]\n",
    "                if owner not in self.visited_pages:\n",
    "                    self.visited_pages[owner] = {}\n",
    "                if repo not in self.visited_pages[owner]:\n",
    "                    self.visited_pages[owner][repo] = set()\n",
    "                self.visited_pages[owner][repo].add(int(page))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {self.filename}: {str(e)}\")\n",
    "            self.analyzed_issues = {}\n",
    "            self.visited_pages = {}\n",
    "    \n",
    "    def save_issue(self, owner: str, repo: str, issue_id: int, description: str, prompt: str, url: str, pr_url: str = \"void\"):\n",
    "        if owner not in self.analyzed_issues:\n",
    "            self.analyzed_issues[owner] = {}\n",
    "        if repo not in self.analyzed_issues[owner]:\n",
    "            self.analyzed_issues[owner][repo] = []\n",
    "        \n",
    "        if str(issue_id) in self.analyzed_issues[owner][repo]:\n",
    "            return False\n",
    "        \n",
    "        self.analyzed_issues[owner][repo].append(str(issue_id))\n",
    "        \n",
    "        try:\n",
    "            workbook = load_workbook(self.filename)\n",
    "            sheet = workbook.active\n",
    "            \n",
    "            sheet.append([\n",
    "                str(issue_id), \n",
    "                owner, \n",
    "                repo, \n",
    "                description, \n",
    "                prompt, \n",
    "                url, \n",
    "                \"REVIEW\",  \n",
    "                \"\",        \n",
    "                pr_url   \n",
    "            ])\n",
    "            \n",
    "            workbook.save(self.filename)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to {self.filename}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def is_analyzed(self, owner: str, repo: str, issue_id: int) -> bool:\n",
    "        return str(issue_id) in self.analyzed_issues.get(owner, {}).get(repo, [])\n",
    "    \n",
    "    def mark_page_as_visited(self, owner: str, repo: str, page: int):\n",
    "        if owner not in self.visited_pages:\n",
    "            self.visited_pages[owner] = {}\n",
    "        if repo not in self.visited_pages[owner]:\n",
    "            self.visited_pages[owner][repo] = set()\n",
    "        \n",
    "        if page in self.visited_pages[owner][repo]:\n",
    "            return\n",
    "        \n",
    "        self.visited_pages[owner][repo].add(page)\n",
    "        \n",
    "        try:\n",
    "            workbook = load_workbook(self.filename)\n",
    "            \n",
    "            if 'Visited Pages' in workbook.sheetnames:\n",
    "                visited_pages_sheet = workbook['Visited Pages']\n",
    "            else:\n",
    "                visited_pages_sheet = workbook.create_sheet('Visited Pages')\n",
    "                visited_pages_sheet.append(['repo_owner', 'repo_name', 'page_number'])\n",
    "            \n",
    "            visited_pages_sheet.append([owner, repo, page])\n",
    "            workbook.save(self.filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving visited page to {self.filename}: {str(e)}\")\n",
    "    \n",
    "    def is_page_visited(self, owner: str, repo: str, page: int) -> bool:\n",
    "        return page in self.visited_pages.get(owner, {}).get(repo, set())\n",
    "    \n",
    "    def get_visited_pages(self, owner: str, repo: str) -> Set[int]:\n",
    "        return self.visited_pages.get(owner, {}).get(repo, set())\n",
    "\n",
    "storage = AnalyzedIssuesStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fcfccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_github_issues(owner: str, repo: str, page: int = 1, per_page: int = 10):\n",
    "    if not GITHUB_TOKEN:\n",
    "        raise ValueError(\"GITHUB_TOKEN environment variable not set\")\n",
    "        \n",
    "    headers = {\n",
    "        'Authorization': f'token {GITHUB_TOKEN}',\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "    }\n",
    "    params = {\n",
    "        'q': f'repo:{owner}/{repo} is:issue is:closed linked:pr',\n",
    "        'page': page,\n",
    "        'per_page': per_page\n",
    "    }\n",
    "    response = requests.get(f'{GITHUB_API_URL}/search/issues', headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ae4b59d-71a2-49b7-bbdb-a03934846463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_issue_description_and_prompt(issue_data: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    title = issue_data.get('title', '')\n",
    "    body = issue_data.get('body', '') or ''\n",
    "    if len(body) > 1000: \n",
    "        body = body[:1000] + \"...\"\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert in analyzing technical issues in code repositories. \n",
    "    Your task is to analyze GitHub issues and provide:\n",
    "\n",
    "    1. A concise description (maximum 5 lines) summarizing the problem stated in the issue.\n",
    "    2. A well-structured prompt (maximum 10 lines) that could be used to request a solution to this issue from a language model. This prompt should request: identification of the files where the problem is located and a concise explanation of the error.\n",
    "        \n",
    "\n",
    "    Respond in JSON format with the keys \"description\" and \"prompt\".\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Analyze the following GitHub issue:\n",
    "\n",
    "    Title: {title}\n",
    "\n",
    "    Description:\n",
    "    {body}\n",
    "\n",
    "    Generate:\n",
    "    1. A concise description (maximum 5 lines) summarizing the problem.\n",
    "    2. An optimized prompt (maximum 10 lines) to request a solution for this issue.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ]\n",
    "    \n",
    "    output_parser = JsonOutputParser()\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        parsed_response = output_parser.invoke(response.content)\n",
    "        return parsed_response.get(\"description\", \"\"), parsed_response.get(\"prompt\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description and prompt: {str(e)}\")\n",
    "        description = f\"Issue related to: {title}\"\n",
    "        prompt = f\"Â¿How to solve the problem '{title}' in a GitHub repository\"\n",
    "        return description, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a5f1b4d-f551-4ba8-addb-67bec98362eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def issue_has_linked_pr(issue_data: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if an issue has a linked pull request.\n",
    "    \n",
    "    Args:\n",
    "        issue_data: The issue data from GitHub API\n",
    "        \n",
    "    Returns:\n",
    "        True if the issue has at least one linked PR, False otherwise\n",
    "    \"\"\"\n",
    "    if \"pull_request\" in issue_data:\n",
    "        return True\n",
    "    \n",
    "    body = issue_data.get('body', '') or ''\n",
    "    \n",
    "    import re\n",
    "    pr_patterns = [\n",
    "        r'(?i)PR\\s+#\\d+',                          # PR #123\n",
    "        r'(?i)pull request\\s+#\\d+',                # pull request #123\n",
    "        r'(?i)linked\\s+to\\s+#\\d+',                 # linked to #123\n",
    "        r'(?i)closes\\s+#\\d+',                      # closes #123\n",
    "        r'(?i)resolves\\s+#\\d+',                    # resolves #123\n",
    "        r'(?i)github\\.com/[^/]+/[^/]+/pull/\\d+',   # Direct PR URL\n",
    "        r'(?i)linked\\s+pull\\s+request'             # mentions of a linked PR\n",
    "    ]\n",
    "    \n",
    "    for pattern in pr_patterns:\n",
    "        if re.search(pattern, body):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_linked_pr_url(issue_data: Dict[str, Any], owner: str, repo: str, headers: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Gets the URL of the first pull request directly linked to an issue.\n",
    "    Only retrieves PRs that are formally linked to the issue.\n",
    "    \n",
    "    Args:\n",
    "        issue_data: Issue data obtained from the GitHub API\n",
    "        owner: Repository owner\n",
    "        repo: Repository name\n",
    "        headers: Headers for GitHub API requests\n",
    "        \n",
    "    Returns:\n",
    "        URL of the first linked PR or \"void\" if none exists\n",
    "    \"\"\"\n",
    "    issue_number = issue_data.get('number')\n",
    "    \n",
    "    if \"pull_request\" in issue_data:\n",
    "        return \"void\"\n",
    "    \n",
    "    try:\n",
    "        # The most reliable way to find directly linked PRs is through the timeline API\n",
    "        timeline_url = f'{GITHUB_API_URL}/repos/{owner}/{repo}/issues/{issue_number}/timeline'\n",
    "        headers_with_preview = headers.copy()\n",
    "        headers_with_preview['Accept'] = 'application/vnd.github.mockingbird-preview+json'\n",
    "        \n",
    "        response = requests.get(timeline_url, headers=headers_with_preview)\n",
    "        if response.status_code == 200:\n",
    "            timeline = response.json()\n",
    "            \n",
    "            for event in timeline:\n",
    "                if event.get('event') == 'connected':\n",
    "                    source = event.get('source', {})\n",
    "                    if source.get('type') == 'pull_request' and source.get('issue', {}).get('pull_request'):\n",
    "                        return source.get('issue', {}).get('html_url', \"void\")\n",
    "                \n",
    "                if event.get('event') == 'cross-referenced':\n",
    "                    source = event.get('source', {})\n",
    "                    if source.get('type') == 'pull_request' and source.get('issue', {}).get('pull_request'):\n",
    "                        # Check if this PR has a closing reference to our issue\n",
    "                        pr_number = source.get('issue', {}).get('number')\n",
    "                        pr_url = f'{GITHUB_API_URL}/repos/{owner}/{repo}/pulls/{pr_number}'\n",
    "                        \n",
    "                        pr_response = requests.get(pr_url, headers=headers)\n",
    "                        if pr_response.status_code == 200:\n",
    "                            pr_data = pr_response.json()\n",
    "                            body = pr_data.get('body', '') or ''\n",
    "                            \n",
    "                            # Check if this PR explicitly closes the \n",
    "                            closes_patterns = [\n",
    "                                f\"closes #{issue_number}\",\n",
    "                                f\"fixes #{issue_number}\",\n",
    "                                f\"resolves #{issue_number}\"\n",
    "                            ]\n",
    "                            \n",
    "                            for pattern in closes_patterns:\n",
    "                                if pattern.lower() in body.lower():\n",
    "                                    return source.get('issue', {}).get('html_url', \"void\")\n",
    "            \n",
    "            for event in timeline:\n",
    "                if event.get('event') == 'closed' and event.get('commit_id'):\n",
    "                    commit_sha = event.get('commit_id')\n",
    "                    commit_url = f'{GITHUB_API_URL}/repos/{owner}/{repo}/commits/{commit_sha}'\n",
    "                    \n",
    "                    commit_response = requests.get(commit_url, headers=headers)\n",
    "                    if commit_response.status_code == 200:\n",
    "                        commit_data = commit_response.json()\n",
    "                        commit_message = commit_data.get('commit', {}).get('message', '')\n",
    "                        \n",
    "                        if f\"#{issue_number}\" in commit_message:\n",
    "                            pr_search_url = f'{GITHUB_API_URL}/search/issues'\n",
    "                            pr_params = {\n",
    "                                'q': f'repo:{owner}/{repo} is:pr {commit_sha}'\n",
    "                            }\n",
    "                            \n",
    "                            pr_response = requests.get(pr_search_url, headers=headers, params=pr_params)\n",
    "                            if pr_response.status_code == 200:\n",
    "                                pr_data = pr_response.json()\n",
    "                                if pr_data.get('total_count', 0) > 0:\n",
    "                                    return pr_data.get('items', [])[0].get('html_url', \"void\")\n",
    "        \n",
    "        search_url = f'{GITHUB_API_URL}/search/issues'\n",
    "        search_query = f'repo:{owner}/{repo} is:pr \"Closes #{issue_number}\"'\n",
    "        params = {'q': search_query}\n",
    "        \n",
    "        search_response = requests.get(search_url, headers=headers, params=params)\n",
    "        if search_response.status_code == 200:\n",
    "            search_result = search_response.json()\n",
    "            if search_result.get('total_count', 0) > 0:\n",
    "                return search_result.get('items', [])[0].get('html_url', \"void\")\n",
    "        \n",
    "        for keyword in [\"Fixes\", \"Resolves\"]:\n",
    "            search_query = f'repo:{owner}/{repo} is:pr \"{keyword} #{issue_number}\"'\n",
    "            params = {'q': search_query}\n",
    "            \n",
    "            search_response = requests.get(search_url, headers=headers, params=params)\n",
    "            if search_response.status_code == 200:\n",
    "                search_result = search_response.json()\n",
    "                if search_result.get('total_count', 0) > 0:\n",
    "                    return search_result.get('items', [])[0].get('html_url', \"void\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting linked PR for issue #{issue_number}: {str(e)}\")\n",
    "    \n",
    "    return \"void\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_github_issues(owner: str, repo: str, target_issues: int, per_page: int = 10) -> int:\n",
    "    \"\"\"\n",
    "    Main function to analyze GitHub issues.\n",
    "\n",
    "    Args:\n",
    "        owner: Repository owner\n",
    "        repo: Repository name\n",
    "        target_issues: Target number of issues to analyze\n",
    "        per_page: Number of issues to retrieve per page\n",
    "\n",
    "    Returns:\n",
    "        Number of issues analyzed\n",
    "    \"\"\"\n",
    "    analyzed_count = 0\n",
    "    current_page = 1\n",
    "    visited_pages = storage.get_visited_pages(owner, repo)\n",
    "    \n",
    "    if not GITHUB_TOKEN:\n",
    "        raise ValueError(\"GITHUB_TOKEN environment variable not set\")\n",
    "        \n",
    "    headers = {\n",
    "        'Authorization': f'token {GITHUB_TOKEN}',\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "    }\n",
    "    \n",
    "    if visited_pages:\n",
    "        current_page = max(visited_pages) + 1\n",
    "    \n",
    "    while analyzed_count < target_issues:\n",
    "        if storage.is_page_visited(owner, repo, current_page):\n",
    "            current_page += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"Fetching page {current_page} for {owner}/{repo}...\")\n",
    "            response_data = fetch_github_issues(owner, repo, current_page, per_page)\n",
    " \n",
    "            storage.mark_page_as_visited(owner, repo, current_page)\n",
    " \n",
    "            issues = response_data.get('items', [])\n",
    "            \n",
    "            if not issues:\n",
    "                print(f\"No more issues found for {owner}/{repo}\")\n",
    "                break\n",
    "            \n",
    "            for issue in issues:\n",
    "                issue_number = issue.get('number')\n",
    "                if not issue_number or storage.is_analyzed(owner, repo, issue_number):\n",
    "                    continue\n",
    "                \n",
    "                has_pr = False\n",
    "                \n",
    "                if issue_has_linked_pr(issue):\n",
    "                    has_pr = True\n",
    "                \n",
    "                if not has_pr:\n",
    "                    print(f\"Skipping issue #{issue_number} - No linked PR found\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Issue #{issue_number} has linked PR(s)\")\n",
    "                \n",
    "                description, prompt = generate_issue_description_and_prompt(issue)\n",
    "                \n",
    "                linked_pr_url = get_linked_pr_url(issue, owner, repo, headers)\n",
    "                print(f\"Linked PR URL for issue #{issue_number}: {linked_pr_url}\")\n",
    "                \n",
    "                url = issue.get('html_url', '')\n",
    "                saved = storage.save_issue(owner, repo, issue_number, description, prompt, url, linked_pr_url)\n",
    "                \n",
    "                if saved:\n",
    "                    analyzed_count += 1\n",
    "                    print(f\"Analyzed issue #{issue_number} - Total: {analyzed_count}/{target_issues}\")\n",
    "                \n",
    "                if analyzed_count >= target_issues:\n",
    "                    break\n",
    "            \n",
    "            current_page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {current_page}: {str(e)}\")\n",
    "            current_page += 1\n",
    "            \n",
    "            # If we've had too many errors, better stop\n",
    "            if current_page > 100:  # Arbitrary limit\n",
    "                print(\"Too many errors, stopping\")\n",
    "                break\n",
    "    \n",
    "    return analyzed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae5f54f4-1573-4077-853e-f10814e21b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 2 for vercel/next.js...\n",
      "Issue #77253 has linked PR(s)\n",
      "Linked PR URL for issue #77253: https://github.com/vercel/next.js/pull/77395\n",
      "Analyzed issue #77253 - Total: 1/5\n",
      "Skipping issue #77232 - No linked PR found\n",
      "Skipping issue #77192 - No linked PR found\n",
      "Skipping issue #77175 - No linked PR found\n",
      "Skipping issue #77164 - No linked PR found\n",
      "Fetching page 3 for vercel/next.js...\n",
      "Skipping issue #77126 - No linked PR found\n",
      "Skipping issue #77083 - No linked PR found\n",
      "Skipping issue #77073 - No linked PR found\n",
      "Skipping issue #76980 - No linked PR found\n",
      "Skipping issue #76925 - No linked PR found\n",
      "Fetching page 4 for vercel/next.js...\n",
      "Skipping issue #76810 - No linked PR found\n",
      "Skipping issue #76747 - No linked PR found\n",
      "Skipping issue #76633 - No linked PR found\n",
      "Skipping issue #76632 - No linked PR found\n",
      "Skipping issue #76581 - No linked PR found\n",
      "Fetching page 5 for vercel/next.js...\n",
      "Skipping issue #76497 - No linked PR found\n",
      "Skipping issue #76275 - No linked PR found\n",
      "Issue #75956 has linked PR(s)\n",
      "Linked PR URL for issue #75956: https://github.com/vercel/next.js/pull/75973\n",
      "Analyzed issue #75956 - Total: 2/5\n",
      "Skipping issue #75938 - No linked PR found\n",
      "Skipping issue #75904 - No linked PR found\n",
      "Fetching page 6 for vercel/next.js...\n",
      "Skipping issue #75895 - No linked PR found\n",
      "Skipping issue #75821 - No linked PR found\n",
      "Skipping issue #75818 - No linked PR found\n",
      "Skipping issue #75807 - No linked PR found\n",
      "Skipping issue #75697 - No linked PR found\n",
      "Fetching page 7 for vercel/next.js...\n",
      "Skipping issue #75526 - No linked PR found\n",
      "Skipping issue #75318 - No linked PR found\n",
      "Skipping issue #75313 - No linked PR found\n",
      "Issue #75189 has linked PR(s)\n",
      "Linked PR URL for issue #75189: https://github.com/vercel/next.js/pull/75190\n",
      "Analyzed issue #75189 - Total: 3/5\n",
      "Skipping issue #74900 - No linked PR found\n",
      "Fetching page 8 for vercel/next.js...\n",
      "Skipping issue #74861 - No linked PR found\n",
      "Skipping issue #74855 - No linked PR found\n",
      "Skipping issue #74843 - No linked PR found\n",
      "Skipping issue #74826 - No linked PR found\n",
      "Issue #74757 has linked PR(s)\n",
      "Linked PR URL for issue #74757: https://github.com/vercel/next.js/pull/77090\n",
      "Analyzed issue #74757 - Total: 4/5\n",
      "Fetching page 9 for vercel/next.js...\n",
      "Skipping issue #74731 - No linked PR found\n",
      "Skipping issue #74664 - No linked PR found\n",
      "Skipping issue #74642 - No linked PR found\n",
      "Skipping issue #74613 - No linked PR found\n",
      "Skipping issue #74547 - No linked PR found\n",
      "Fetching page 10 for vercel/next.js...\n",
      "Skipping issue #74422 - No linked PR found\n",
      "Skipping issue #74385 - No linked PR found\n",
      "Skipping issue #74265 - No linked PR found\n",
      "Skipping issue #74190 - No linked PR found\n",
      "Issue #74181 has linked PR(s)\n",
      "Linked PR URL for issue #74181: https://github.com/vercel/next.js/pull/74179\n",
      "Analyzed issue #74181 - Total: 5/5\n",
      "Successfully analyzed 5 issues from vercel/next.js\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    owner = 'vercel' \n",
    "    repo = 'next.js' \n",
    "    target_issues = 5\n",
    "    per_page = 5\n",
    "\n",
    "    try:\n",
    "        if not GITHUB_TOKEN:\n",
    "            print(\"ERROR: GITHUB_TOKEN environment variable not set\")\n",
    "            print(\"Please set it with: export GITHUB_TOKEN=your_token_here\")\n",
    "            return\n",
    "\n",
    "        # Run the analysis\n",
    "        analyzed_count = analyze_github_issues(owner, repo, target_issues, per_page)\n",
    "        print(f\"Successfully analyzed {analyzed_count} issues from {owner}/{repo}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: {str(e)}')\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b22194-54a7-41a8-ad15-1e76b42351f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
